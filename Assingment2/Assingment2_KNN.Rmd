---
title: "Assingment2_KNN"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning  = FALSE)
```


#getting all the library we might require
``` {r}

library(e1071)
library(ISLR)
library(caret)
library(FNN)
library(gmodels)
```

#Reading the Universal bank CSV file 
``` {r}
UniversalBank <- read.csv("A:\\DATA_SETS\\UniversalBank.csv")

```
#removing the attributes ID and ZIPCODE from the dataset
``` {r}
UniversalBankNew <- UniversalBank[c(-1,-5)]
``` 

#Building a dummy for the Education column using the Dummies kit 
``` {r}

library(dummies)



#Making dummies
EducationDummy <- dummy(UniversalBankNew$Education)


temp <- cbind(UniversalBankNew,EducationDummy)


#Also removing the education colm
UniversalBankNew1 <- temp[c(-6)]



set.seed(1234)
``` 

#Divide the data into sets of training (60 %) and validation (40 %).
``` {r}
Index <- createDataPartition(UniversalBankNew1$Income, p = 0.6, list = FALSE)
trainingData <- UniversalBankNew1[Index,]

dim(trainingData)
validationData <- UniversalBankNew1[-Index,]

dim(validationData)

``` 


# Question 1.) Age = 40, Experience = 10, Income = 84, Family = 2, CCAvg = 2, Education_1 = 0, Education_2 = 1, Education_3 = 0, Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1, and Credit Card = 1. Perform a k-NN classification with all predictors except ID and ZIP code using k = 1. Remember to transform categorical predictors with more than two categories into dummy variables first. Specify the success class as 1 (loan acceptance), and use the default cutoff value of 0.5. How would this customer be classified?


#before using this data we need to standardize the data using normalization
``` {r}


norm_values <- preProcess(trainingData, method = c ("center", "scale"))

trainigNormalize <- predict(norm_values, trainingData)
validationNormalize <- predict(norm_values, validationData)
Total_normal <- predict(norm_values, UniversalBankNew1)



``` 
#Predictors and prediction data are distinguished.The Personal Loan Variable here is the Predictive Data and factorizes the values as Reject and Approve instead of 0 and 1, while all the other variables from the dataset are the Predictors.

``` {r}


Training_Data <- trainigNormalize[,-7]
Training_Outcome <- factor(trainingData[,7], levels = c(0,1), labels = c("Deny","Accept"))

Validation_Data <- validationNormalize[,-7]
Validation_Outcome <- factor(validationData[,7], levels = c(0,1), labels = c("Deny", "Accept"))

Total_Data <- Total_normal[,-7]
Total_Outcome <- factor(UniversalBankNew1[,7], levels = c(0,1), labels = c("Deny", "Accept"))
``` 



``` {r}
TesTraining_Data <- c(40, 10, 84, 2, 2, 0, 0, 0, 1, 1, 0, 1, 0)
knn_test <- knn(Training_Data, TesTraining_Data, cl=Training_Outcome, k=1,prob = 0.5)
knn_test
```


# Question 2.) What is a choice of k that balances between overfitting and ignoring the predictor information?
``` {r}
 #For best observation value of K we'll take the square root of the total data set i.e. 3002 and 
#recusrivelly try to find the best fit.


sq.value<- round(sqrt(3002)) 
sq.value
#creating a data frame for checking all the accuracy for all the sq.value
accuracy <- data.frame(k = seq(1,sq.value,1), accuracy = rep (0, sq.value))


for (i in 1:sq.value){
  knn.pred <- knn(Training_Data, Validation_Data, cl=Training_Outcome, k=i)
  accuracy[i,2] <- confusionMatrix(knn.pred, Validation_Outcome)$overall[1]
}


best_fit <- accuracy[which.max(accuracy$accuracy),]
best_fit

```

# Question 3.)Show the confusion matrix for the validation data that results from using the best k.
``` {r}
knn.pred <- knn(Training_Data, Validation_Data, cl=Training_Outcome, k=best_fit$k, prob = TRUE)

CrossTable(Validation_Outcome, knn.pred)
``` 

# Question 4.) Consider the following customer: Age = 40, Experience = 10, Income = 84, Family = 2, CCAvg = 2, Education_1 = 0, Education_2 = 1, Education_3 = 0, Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1 and Credit Card = 1. Classify the customer using the best k.
``` {r}
TesTraining_Data <- c(40, 10, 84, 2, 2, 0, 0, 0, 1, 1, 0, 1, 0)
bestFitKnn <- knn(Training_Data, TesTraining_Data, cl=Training_Outcome, k =best_fit$k, prob = TRUE)
(bestFitKnn)

#Using the Complete Data Set and then applying the model is the optimal solution for the KNN model.

totalKnn <- knn(Training_Data,Total_Data, cl=Training_Outcome, k= best_fit$k, prob = TRUE)
CrossTable(Total_Outcome, totalKnn)
```  
# Question 5.) Repartition the data, this time into training, validation, and test sets (50% : 30% : 20%). Apply the k-NN method with the k chosen above. Compare the confusion matrix of the test set with that of the training and validation sets. Comment on the differences and their reason. 

#partition into 50%,30%,20% into training, validation, and test sets

```{r}
IndexNew <- createDataPartition(UniversalBankNew1$Income, p = 0.5, list = FALSE)
trainingData2 = UniversalBankNew1[IndexNew,]

Nextdata <- UniversalBankNew1[-IndexNew,]

Indexnew2 <- createDataPartition(Nextdata$Income, p = 0.6, list = FALSE)
validationData2 = Nextdata[Indexnew2,]

testData2 <- Nextdata[-Indexnew2,]


#before using this data we need to standardize the data using normalization

norm_values2 <- preProcess(trainingData2, method = c ("center", "scale"))

trainigNormalize2 <- predict(norm_values2, trainingData2)

validationNormalize2 <- predict(norm_values2, validationData2)

testNormalize2 <- predict(norm_values2, testData2)

Total_normal2 <- predict(norm_values2, UniversalBankNew1)




Training_Data2 <- trainigNormalize2[,-7]
Training_Outcome2 <- factor(trainingData2[,7], levels = c(0,1), labels = c("Deny","Accept"))

Validation_Data2 <- validationNormalize2[,-7]
Validation_Outcome2 <- factor(validationData2[,7], levels = c(0,1), labels = c("Deny", "Accept"))

TesTraining_Data2 <- testNormalize2[,-7]
TesTraining_Outcome2 <- factor(testData2[,7], levels = c(0,1), labels = c("Deny", "Accept"))

Total_Data2 <- Total_normal2[,-7]
Total_Outcome2 <- factor(UniversalBankNew1[,7], levels = c(0,1), labels = c("Deny", "Accept"))

#Apply KNN with the optimum k value (k=4) to the Training and Validation set, later to the Training and Test set, and finally to the entire dataset.

#validation
Knn_validation <- knn(Training_Data2, Validation_Data2, cl=Training_Outcome2, k= best_fit$k, prob = TRUE)
CrossTable(Validation_Outcome2,Knn_validation, prop.chisq = FALSE)
#test
Knn_testing <- knn(Training_Data2, TesTraining_Data2, cl=Training_Outcome2, k =best_fit$k, prob = TRUE)
CrossTable(TesTraining_Outcome2,Knn_testing, prop.chisq = FALSE)
#accuracy 

#total
Knn_total <- knn(Training_Data2, Total_Data2, cl=Training_Outcome2, k =best_fit$k, prob = TRUE)
CrossTable(Total_Outcome2,Knn_total, prop.chisq = FALSE)
``` 

#COMMENTS: When we apply the KNN model to various datasets, we can see that the significance of accuracy continues to change. The precision shift may be due to the partition being biased i.e. into 3 sets of Training into 50 %, Validation into 30% and rest 20% for test set. The data in each of the partitioned datasets is different, and due to the type of attribute we chose to partition the dataset, bias will occur.

